{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import nlp_project_functions as functions\n",
    "\n",
    "dir = \"data/data_preprocessed\"\n",
    "\n",
    "orgel_list = []\n",
    "aba_list = []\n",
    "inside_per = False\n",
    "inside_loc = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"data/data_preprocessed/\"):\n",
    "    if filename.endswith('txt'):\n",
    "        with open(os.path.join(dir, filename)) as f:\n",
    "            sermon = f.read()\n",
    "        \n",
    "        sermon = functions.sermon_cleanup(sermon)\n",
    "\n",
    "        # add spaces around tags\n",
    "        sermon = re.sub(r'</(PERSON|LOCATION)>(\\S)', r'</\\1> \\2', sermon)\n",
    "        sermon = re.sub(r'(\\S)<(PERSON|LOCATION)>', r'\\1 <\\2>', sermon)\n",
    "\n",
    "        # add spaces around punctuation\n",
    "        sermon = re.sub(r'(,|;|:|\\.|\\?|!|\\))', r' \\1 ', sermon)\n",
    "        sermon = re.sub(r'(\\()', r' \\1 ', sermon)\n",
    "\n",
    "        # filter out long names \n",
    "        long_names = functions.get_long_names(sermon)\n",
    "        for name in long_names:\n",
    "            sermon = re.sub(name, '', sermon)\n",
    "\n",
    "        # create lists of words\n",
    "        words = re.split(r'\\s', sermon)\n",
    "        words = [i for i in words if i != \"\"]\n",
    "        \n",
    "        if filename.startswith(\"E\"):\n",
    "            #orgel_list.append(\"# \" + filename[:7])\n",
    "            for word in words:\n",
    "                orgel_list.append(functions.word_to_row(word))\n",
    "        else:\n",
    "            #aba_list.append(\"# \" + filename)\n",
    "            for word in words:\n",
    "                aba_list.append(functions.word_to_row(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orgelpredigten Tokens: 641,481\n",
      "Abacus Tokens: 157,431\n"
     ]
    }
   ],
   "source": [
    "print(f\"Orgelpredigten Tokens: {len(orgel_list):,}\")\n",
    "print(f\"Abacus Tokens: {len(aba_list):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token[0] for token in orgel_list + aba_list]\n",
    "all_labels = [token[1] for token in orgel_list + aba_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = [token for token in filter(lambda x: x != \"O\", all_labels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,746 of 798,912 tokens (≈1.97%) are named entities.\n"
     ]
    }
   ],
   "source": [
    "all = len(all_tokens)\n",
    "ne = len(named_entities)\n",
    "\n",
    "print(f\"{ne:,} of {all:,} tokens (≈{ne/(all/100):.2f}%) are named entities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [[token, label] for token, label in zip(all_tokens, all_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = \"\"\n",
    "\n",
    "for item in combined:\n",
    "    if item[0] in (\".\", \"!\", \"?\", \":\"):\n",
    "        all_data += f\"{item[0]}\\t{item[1]}\\n\\n\"\n",
    "    else:\n",
    "        all_data += f\"{item[0]}\\t{item[1]}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to file!\n"
     ]
    }
   ],
   "source": [
    "if functions.check_bio_validity(all_data.split(\"\\n\")):\n",
    "    with open('data/train_test_val/all_data.tsv', 'w', encoding='utf8') as f:\n",
    "        f.write(all_data)\n",
    "    print(\"Data written to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current longest sentence: 85\n",
      "Current longest sentence: 91\n",
      "Current longest sentence: 104\n",
      "Current longest sentence: 117\n",
      "Current longest sentence: 132\n",
      "Current longest sentence: 185\n",
      "Current longest sentence: 204\n",
      "Current longest sentence: 212\n",
      "Current longest sentence: 234\n",
      "Current longest sentence: 254\n",
      "Current longest sentence: 288\n",
      "Current longest sentence: 294\n",
      "Current longest sentence: 300\n"
     ]
    }
   ],
   "source": [
    "tokens, labels = functions.read_conll_data('data/train_test_val/all_data.tsv')\n",
    "\n",
    "sentences = []\n",
    "longest = 0\n",
    "for token_i, label_i in zip(tokens, labels):\n",
    "    # throw out 'sentences' under 3 words long\n",
    "    if len(token_i) > 2:\n",
    "        sentence = [[token, label] for token, label in list(zip(token_i,label_i))]\n",
    "        # split up sentences that are over 300 tokens long\n",
    "        if len(sentence) <= 300:\n",
    "            sentences.append(sentence)\n",
    "        else:\n",
    "            list_of_sents = functions.find_good_split(sentence, index=int(len(sentence)/2))\n",
    "            sentences.extend(list_of_sents)\n",
    "    if len(sentences[-1]) > longest:\n",
    "        longest = len(sentences[-1])\n",
    "        print(f\"Current longest sentence: {longest}\")\n",
    "\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sentences[:int(len(sentences) * 0.2)]\n",
    "train = sentences[int(len(sentences) * 0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 29299\n",
      "Sentences in training data: 23440 (= 80.003%)\n",
      "Sentences in test data: 5859 (= 19.997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of sentences: {len(sentences)}\")\n",
    "print(f\"Sentences in training data: {len(train)} (= {100/(len(sentences)/len(train)):.3f}%)\")\n",
    "print(f\"Sentences in test data: {len(test)} (= {100/(len(sentences)/len(test)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_d_list_to_conll(list):\n",
    "    string = \"\"\n",
    "    for id, sentence in enumerate(list):\n",
    "        for idx, item in enumerate(sentence):\n",
    "            try:\n",
    "                string += f\"{item[0]}\\t{item[1]}\\n\"\n",
    "            except:\n",
    "                print(f\"Problem in sentence {id}, token {idx}.\")\n",
    "        string += \"\\n\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tsv = two_d_list_to_conll(train)\n",
    "test_tsv = two_d_list_to_conll(test)\n",
    "\n",
    "with open('data/train_test_val/train.tsv', 'w', encoding='utf8') as f:\n",
    "    f.write(train_tsv)\n",
    "\n",
    "with open('data/train_test_val/test.tsv', 'w', encoding='utf8') as f:\n",
    "    f.write(test_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Sentence boundaries found, automatic sentence segmentation with `-s`\n",
      "disabled.\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (2344 documents):\n",
      "data/train_test_val/train.spacy\u001b[0m\n",
      "\u001b[38;5;3m⚠ Sentence boundaries found, automatic sentence segmentation with `-s`\n",
      "disabled.\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (586 documents):\n",
      "data/train_test_val/test.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# List of files to iterate over\n",
    "files=(\"train.tsv\" \"test.tsv\")\n",
    "\n",
    "# Loop through each file\n",
    "for f in \"${files[@]}\"\n",
    "do\n",
    "    python -m spacy convert ./data/train_test_val/$f ./data/train_test_val/ -c conll -s -n 10\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
